---
layout:     post
title:      机器学习
subtitle:   xgboost调参经验
date:       2018-03-14
author:     john
## header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 机器学习
    - xgboost
---
### xgboost参数分类
xgboost的参数总的来说可以分为三类:
1. 通用参数: 指导函数总体功能
2. Booster参数: 在每一步指导独立的提升器
3. 学习目标参数: 指导如何优化

#### 通用参数
1. booster[default==gbtree]
   - gbtree: 基于树的模型
   - gblinear: 线性模型
2. slient[default==0]
   - 当这个参数为1，静默模式开启，不会输出任何信息
   - 这个参数保持默认的0，这样可以帮助我们更好地理解模型
3. nthread[默认值为最大可能的线程数]
   - 这个参数用来进行多线程控制，应当输入系统的核数
   - 如果你使用CPU全部的核，就不要输入这个参数，算法会自动检测它

#### booster参数
尽管有两种booster可以选择，这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。
1. eta[默认0.3]
- 和GBM中的learning rate参数类似
- 通过减少每一步的权重，提高模型的鲁棒性
- 典型值为0.01-0.2

2. min_child_weight[默认1]
- 决定最小叶子节点样本权重和
- 和GBM的min_child_weight参数类似，但不完全一样。XGBOOST的这个参数是最小样本权重的和，而GBM参数是最小样本总数
- 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本
- 如果这个值过高，会导致过拟合。这个参数需要使用CV来调整

3. max_depth[默认6]
- 和GBM中的参数相同，这个值为数的最大深度
- 这个值用来避免过拟合。max_depth越大，模型会学到更具体更局部的样本
- 需要使用CV函数来进行调优
- 典型值: 3-10

4. max_leaf_nodes
- 树上最大的节点或叶子的数量
- 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n平方个叶子
- 如果定义了这个参数，GBM会忽略max_depth参数

5. gamma
- 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值
- 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的

6. max_delta_step
- 这个参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守
- 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的
- 这个参数一般用不到，但是你可以挖掘出来它更多的用处

7. subsample
- 和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例
- 减少这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合
- 典型值: 0.5-1

8. colsample_bytree[默认1]
- 和GBM里面的max_features参数类似。用来控制每颗随机采样的列数的占比(每一列是一个特征)
- 典型值: 0.5-1

9. colsample_bylevel[默认1]
- 用来控制树的每一级的每一次分裂，对列数的采样的占比

10. lambda[默认1]
- 权重的L2正则化项(和Ridge regression类似)

11. alpha[默认1]
- 权重的L1正则化项(和Lasso regression类似)

12. scale_pos_weight[默认1]
- 在各类别样本十分不平衡时，把这个参数设定为一个正数，可以使算法更快收敛

#### 学习目标参数
这个参数用来控制理想的优化目标和每一步结果的度量方法
1. objective[默认reg:linear]

   这个参数定义需要被最小化的损失函数，最常用的值有:
   - binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)
   - multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。
     - 在这种情况下，你需要多设置一个参数: num_classes(类别数据)
   - multi:softprob和multi:softmax参数一样，返回的是每个数据属于各个类别的概率。

2. eval_metric[默认值取决于objective参数的取值]

  - 对于有效数据的度量方法
  - 对于回归问题，默认值是rmse，对于分类问题，默认值是error。
  - 典型值有:
    - rmse 均方根误差
    - mae 平均绝对误差
    - logloss 负对数似然函数值
    - error 二分类错误率(阈值为0.5)
    - merror 多分类错误率
    - mlogloss 多分类logloss损失函数
    - auc 曲线下面积
3. seed(默认为0)
  - 随机数的种子
  - 设置它可以复现随机数据的结果，也可以用于调整参数

### 参数调优的一般方法
1. 选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。
2. 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth，min_child_weight, gamma, subsample, colsample_bytree)。
3. xgboost的正则化参数调优(lambda，alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。
4. 降低学习速率，确定理想参数。

更多详细信息可以参考原文[Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
